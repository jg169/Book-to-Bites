{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "NO47mAC5iGH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install langchain \n",
        "!pip -q install langchain openai==0.27.0  tiktoken "
      ],
      "metadata": {
        "id": "qZ5JXLe1OlLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install bark (make sure you have torch>=2 for much faster flash-attention)\n",
        "!pip install git+https://github.com/suno-ai/bark.git"
      ],
      "metadata": {
        "id": "YkizeTpG_3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "h0agEM-ZkQxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ask for API keys and input text\n",
        "\n",
        "ApiKey = input('what is your API key?')\n",
        "\n",
        "text = input('copy and paste a page that you wanted summarized into a soundbite here')"
      ],
      "metadata": {
        "id": "57m38KzgkijL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trimming and counting tokens"
      ],
      "metadata": {
        "id": "mrN4PAaN7cEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Optional, List\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "ARTICLES_PREPOSITIONS = {\n",
        "    \"english\": ['the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of']\n",
        "}\n",
        "\n",
        "NEGATION_WORDS = {\n",
        "    \"spanish\": [\n",
        "        'no',\n",
        "        'ni',\n",
        "        'nunca',\n",
        "        'jamas',\n",
        "        'tampoco',\n",
        "        'nadie',\n",
        "        'nada',\n",
        "        'ninguno',\n",
        "        'ninguna',\n",
        "        'ningunos',\n",
        "        'ningunas',\n",
        "        'ningun',\n",
        "    ],\n",
        "    \"english\": [\n",
        "        'no',\n",
        "        'nor',\n",
        "        'not',\n",
        "        'don',\n",
        "        \"dont\",\n",
        "        'ain',\n",
        "        'aren',\n",
        "        \"arent\",\n",
        "        'couldn',\n",
        "        \"couldnt\",\n",
        "        'didn',\n",
        "        \"didnt\",\n",
        "        'doesn',\n",
        "        \"doesnt\",\n",
        "        'hadn',\n",
        "        \"hadnt\",\n",
        "        'hasn',\n",
        "        \"hasnt\",\n",
        "        'haven',\n",
        "        \"havent\",\n",
        "        'isn',\n",
        "        \"isnt\",\n",
        "        'mightn',\n",
        "        \"mightnt\",\n",
        "        'mustn',\n",
        "        \"mustnt\",\n",
        "        'needn',\n",
        "        \"neednt\",\n",
        "        'shan',\n",
        "        \"shant\",\n",
        "        'shouldn',\n",
        "        \"shouldnt\",\n",
        "        'wasn',\n",
        "        \"wasnt\",\n",
        "        'weren',\n",
        "        \"werent\",\n",
        "        'won',\n",
        "        \"wont\",\n",
        "        'wouldn',\n",
        "        \"wouldnt\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "PUNCTUATION = [\".\", \",\", \"'\", '\"', \"!\", \"?\", \";\", \":\", \"-\"]\n",
        "\n",
        "def trim(\n",
        "    text: str, stemmer: Optional[str] = None, language: str = \"english\", remove_spaces: bool = True,\n",
        "        remove_stopwords: bool = True, remove_punctuation: bool = True) -> str:\n",
        "\n",
        "    if language not in stopwords.fileids():\n",
        "        raise ValueError(\"Unsupported language\")\n",
        "\n",
        "    accepted_stemmers = (\"snowball\", \"porter\", \"lancaster\")\n",
        "    if stemmer and stemmer not in accepted_stemmers:\n",
        "        raise ValueError(\"Stemmer must be one of\", accepted_stemmers)\n",
        "\n",
        "    # merge contractions\n",
        "    text: str = text.replace(\"'\", \"\").replace(\"â€™\", \"\")\n",
        "\n",
        "    # tokenize words, keep uppercase\n",
        "    tokenized: List = nltk.word_tokenize(text)\n",
        "\n",
        "    if remove_punctuation:\n",
        "        tokenized = [word for word in tokenized if word not in PUNCTUATION]\n",
        "\n",
        "    if remove_stopwords:\n",
        "        nltk_stopwords = stopwords.words(language)\n",
        "        words_to_exclude = set(\n",
        "            nltk_stopwords + ARTICLES_PREPOSITIONS.get(language, [])\n",
        "        ) - set(NEGATION_WORDS.get(language, []))\n",
        "\n",
        "        tokenized = [word for word in tokenized if word.lower() not in words_to_exclude]\n",
        "\n",
        "    words = tokenized\n",
        "\n",
        "    if stemmer:\n",
        "        if stemmer == \"porter\":\n",
        "            stemmer = PorterStemmer()\n",
        "        elif stemmer == \"snowball\":\n",
        "            stemmer = SnowballStemmer(language)\n",
        "        elif stemmer == \"lancaster\":\n",
        "            stemmer = LancasterStemmer()\n",
        "        words = [stemmer.stem(word) for word in tokenized]\n",
        "\n",
        "        # restore title_case and uppercase after stemming\n",
        "        case_restored = []\n",
        "        for i, word in enumerate(words):\n",
        "            if tokenized[i].istitle():\n",
        "                word = word.title()\n",
        "            elif tokenized[i].isupper():\n",
        "                word = word.upper()\n",
        "            case_restored.append(word)\n",
        "\n",
        "        words = case_restored\n",
        "    #delete the last period \n",
        "    words2 = words.pop()\n",
        "\n",
        "    # remove spaces\n",
        "    #join_str = \"\" if remove_spaces else \" \"\n",
        "    #trimmed: str = join_str.join(words).strip()\n",
        "    #if not remove_punctuation:\n",
        "        # this is a hack to remove spaces before punctuation\n",
        "        #trimmed = re.sub(r\"\\s([?.!,:;])\", r\"\\1\", trimmed)\n",
        "        \n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "rAswfSGKonqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text is from the input above\n",
        "trimmed_text = trim(text)\n",
        "print(trimmed_text)"
      ],
      "metadata": {
        "id": "nJjSEcZDo0PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#see if there is a valid number of tokens "
      ],
      "metadata": {
        "id": "nJBWi5Km7jQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tiktoken"
      ],
      "metadata": {
        "id": "TqPU1Kyj7nKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "f8Uew0qs7tBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "gwuQnaS17vKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numTokens = len(encoding.encode(trimmed_text))"
      ],
      "metadata": {
        "id": "L4B5qKtA7y9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just for comparison with original \n",
        "len(encoding.encode(text))"
      ],
      "metadata": {
        "id": "4wk_f72d8bRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if numTokens >= 4096:\n",
        "  print('THIS WILL NOT WORK')\n",
        "else:\n",
        "  print('Go ahead')"
      ],
      "metadata": {
        "id": "rfVt-XiG9rAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Langchain and GPT-3.5 turbo"
      ],
      "metadata": {
        "id": "Crdhphr34Cdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "38ecVXK74Gl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "wukJu-4D4MiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization\n"
      ],
      "metadata": {
        "id": "-cBD2a7COStG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chatGPT = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "hOWxZeP44V1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert at making strong factual summarizations.\\\n",
        "     Take the article submitted by the user and produce a factual useful summary\"),\n",
        "    HumanMessage(content=trimmed_text)\n",
        "]\n",
        "responses = chatGPT(messages)"
      ],
      "metadata": {
        "id": "EJXo5qg04z3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summarizing the text (something with langchain) \n",
        "#summarizedText = responses\n",
        "summarizedText = 'MoviePy is a Python module used for basic video editing operations. It allows you to concatenate multiple video file clips, similar to joining character strings end-to-end. This means that the clips are played one after another, creating a single video file. The size of the clips does not matter, as they will appear centered within the larger clip if they are not big enough to contain it.'\n",
        "print(summarizedText)\n",
        "\n"
      ],
      "metadata": {
        "id": "w6eGOH2jk4kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion to realistic text to speech and read out \n",
        "\n",
        "(can take a while) "
      ],
      "metadata": {
        "id": "cW3KEC4PCA5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "w_GKdjSvR7I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#long form generation\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "from IPython.display import Audio\n",
        "import nltk  # we'll use this to split into sentences\n",
        "import numpy as np\n",
        "\n",
        "from bark.generation import (\n",
        "    generate_text_semantic,\n",
        "    preload_models,\n",
        ")\n",
        "from bark.api import semantic_to_waveform\n",
        "from bark import generate_audio, SAMPLE_RATE\n",
        "\n"
      ],
      "metadata": {
        "id": "Cl01EACYRdgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preload_models()"
      ],
      "metadata": {
        "id": "qGSEorjKkr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script = summarizedText.replace(\"\\n\", \" \").strip()\n",
        "sentences = nltk.sent_tokenize(script)\n",
        "GEN_TEMP = 0.6\n",
        "SPEAKER = \"v2/en_speaker_6\"\n",
        "silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
        "\n",
        "pieces = []\n",
        "for sentence in sentences:\n",
        "    semantic_tokens = generate_text_semantic(\n",
        "        sentence,\n",
        "        history_prompt=SPEAKER,\n",
        "        temp=GEN_TEMP,\n",
        "        min_eos_p=0.05,  # this controls how likely the generation is to end\n",
        "    )\n",
        "\n",
        "    audio_array = semantic_to_waveform(semantic_tokens, history_prompt=SPEAKER,)\n",
        "    pieces += [audio_array, silence.copy()]\n",
        "    Audio(np.concatenate(pieces), rate=SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "ghvztaiTk917"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}