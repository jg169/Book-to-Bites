{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "NO47mAC5iGH7",
        "h0agEM-ZkQxb",
        "mrN4PAaN7cEn",
        "Crdhphr34Cdp",
        "-cBD2a7COStG",
        "cW3KEC4PCA5f"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "NO47mAC5iGH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install langchain \n",
        "!pip -q install langchain openai==0.27.0  tiktoken "
      ],
      "metadata": {
        "id": "qZ5JXLe1OlLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install bark (make sure you have torch>=2 for much faster flash-attention)\n",
        "!pip install git+https://github.com/suno-ai/bark.git"
      ],
      "metadata": {
        "id": "YkizeTpG_3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prompt-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHEmZzoYVecw",
        "outputId": "e93c2d9b-d319-4d00-d61b-2a95cea71be4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting prompt-optimizer\n",
            "  Downloading prompt_optimizer-0.2.1-py3-none-any.whl (26 kB)\n",
            "Collecting autocorrect<3.0.0,>=2.6.1 (from prompt-optimizer)\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting black<24.0.0,>=23.3.0 (from prompt-optimizer)\n",
            "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isort<6.0.0,>=5.12.0 (from prompt-optimizer)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from prompt-optimizer) (3.8.1)\n",
            "Collecting pulp<3.0.0,>=2.7.0 (from prompt-optimizer)\n",
            "  Downloading PuLP-2.7.0-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<0.4.0,>=0.3.3 (from prompt-optimizer)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from prompt-optimizer) (2.0.1+cu118)\n",
            "Collecting transformers<5.0.0,>=4.27.4 (from prompt-optimizer)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black<24.0.0,>=23.3.0->prompt-optimizer) (8.1.3)\n",
            "Collecting mypy-extensions>=0.4.3 (from black<24.0.0,>=23.3.0->prompt-optimizer)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black<24.0.0,>=23.3.0->prompt-optimizer) (23.1)\n",
            "Collecting pathspec>=0.9.0 (from black<24.0.0,>=23.3.0->prompt-optimizer)\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black<24.0.0,>=23.3.0->prompt-optimizer) (3.3.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black<24.0.0,>=23.3.0->prompt-optimizer) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->prompt-optimizer) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->prompt-optimizer) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->prompt-optimizer) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.4.0,>=0.3.3->prompt-optimizer) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->prompt-optimizer) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->prompt-optimizer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->prompt-optimizer) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->prompt-optimizer) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->prompt-optimizer) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->prompt-optimizer) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3.0.0,>=2.0.0->prompt-optimizer) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3.0.0,>=2.0.0->prompt-optimizer) (16.0.5)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<5.0.0,>=4.27.4->prompt-optimizer)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.27.4->prompt-optimizer) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.27.4->prompt-optimizer) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.27.4->prompt-optimizer)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<5.0.0,>=4.27.4->prompt-optimizer) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.4.0,>=0.3.3->prompt-optimizer) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.4.0,>=0.3.3->prompt-optimizer) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.4.0,>=0.3.3->prompt-optimizer) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.4.0,>=0.3.3->prompt-optimizer) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->prompt-optimizer) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.0->prompt-optimizer) (1.3.0)\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=2a9f8de64800862aa94cbcb3cbc99f320f793e645cc2d17d04759af8c7ab8aa3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: tokenizers, pulp, pathspec, mypy-extensions, isort, autocorrect, tiktoken, huggingface-hub, black, transformers, prompt-optimizer\n",
            "Successfully installed autocorrect-2.6.1 black-23.3.0 huggingface-hub-0.15.1 isort-5.12.0 mypy-extensions-1.0.0 pathspec-0.11.1 prompt-optimizer-0.2.1 pulp-2.7.0 tiktoken-0.3.3 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tiktoken"
      ],
      "metadata": {
        "id": "gN56MW2GbiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "h0agEM-ZkQxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ask for API keys and input text\n",
        "\n",
        "ApiKey = input('what is your API key?')\n",
        "\n",
        "text = input('copy and paste a page that you wanted summarized into a soundbite here')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57m38KzgkijL",
        "outputId": "d53c36f8-3f1c-4cca-a23d-7f686cd43c85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is your API key?1234\n",
            "copy and paste a page that you wanted summarized into a soundbite hereThe 2023 Indian Premier League (also known as Tata IPL 2023 for sponsorship reasons and sometimes referred to as IPL 2023 or IPL 16) was the 16th season of the Indian Premier League, a franchise Twenty20 cricket league in India. It is organised by the Board of Control for Cricket in India.[1]  In the final, Chennai Super Kings defeated Gujarat Titans, by five wickets (DLS method) to win their record fifth league title.[2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trimming and counting tokens"
      ],
      "metadata": {
        "id": "mrN4PAaN7cEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new prompt optimizier for v0.1.0\n",
        "from prompt_optimizer.poptim import EntropyOptim\n"
      ],
      "metadata": {
        "id": "RINWNhb2Va1B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#semantically trims prompt with minimal loss of semantic value before trimming \n",
        "prompt = text\n",
        "p_optimizer = EntropyOptim(verbose=True, p=0.1) #reccomended: p = 0.1 represents an 11% reduction in tokens while reducing logiQA accuracy by only 6%; other options such as p = 0.05, 0.25, 0.5 are available(0.5 not reccomended)\n",
        "optimized_prompt = p_optimizer(prompt)\n",
        "optimized_prompt_output = optimized_prompt['content']\n",
        "text = optimized_prompt_output\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOMDPcnWWeFx",
        "outputId": "98b24520-dde6-47b8-962c-343c0d69d6ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2023-06-07 15:53:33,580 - prompt_optimizer.poptim.logger - INFO - Prompt Data Before: The 2023 Indian Premier League (also known as Tata IPL 2023 for sponsorship reasons and sometimes referred to as IPL 2023 or IPL 16) was the 16th season of the Indian Premier League, a franchise Twenty20 cricket league in India. It is organised by the Board of Control for Cricket in India.[1]  In the final, Chennai Super Kings defeated Gujarat Titans, by five wickets (DLS method) to win their record fifth league title.[2]\n",
            "INFO:prompt_optimizer.poptim.logger:Prompt Data Before: The 2023 Indian Premier League (also known as Tata IPL 2023 for sponsorship reasons and sometimes referred to as IPL 2023 or IPL 16) was the 16th season of the Indian Premier League, a franchise Twenty20 cricket league in India. It is organised by the Board of Control for Cricket in India.[1]  In the final, Chennai Super Kings defeated Gujarat Titans, by five wickets (DLS method) to win their record fifth league title.[2]\n",
            "2023-06-07 15:53:33,584 - prompt_optimizer.poptim.logger - INFO - Prompt Data After: The 2023 Indian Premier League ( also known Tata IPL 2023 for sponsorship reasons and sometimes referred to as IPL 2023 or IPL 16 ) was the 16th season Indian Premier, a franchise Twenty20 cricket league in India. is organised the Board Control for Cricket in. [ 1 ] In the final, Super Kings defeated Gujarat Titans, by five wickets ( DLS method ) win their record fifth league title. [ 2 ]\n",
            "INFO:prompt_optimizer.poptim.logger:Prompt Data After: The 2023 Indian Premier League ( also known Tata IPL 2023 for sponsorship reasons and sometimes referred to as IPL 2023 or IPL 16 ) was the 16th season Indian Premier, a franchise Twenty20 cricket league in India. is organised the Board Control for Cricket in. [ 1 ] In the final, Super Kings defeated Gujarat Titans, by five wickets ( DLS method ) win their record fifth league title. [ 2 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2023 Indian Premier League ( also known Tata IPL 2023 for sponsorship reasons and sometimes referred to as IPL 2023 or IPL 16 ) was the 16th season Indian Premier, a franchise Twenty20 cricket league in India. is organised the Board Control for Cricket in. [ 1 ] In the final, Super Kings defeated Gujarat Titans, by five wickets ( DLS method ) win their record fifth league title. [ 2 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt trimming inspired by GPTrim\n",
        "import re\n",
        "from typing import Optional, List\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o59Wfxx_Ywmg",
        "outputId": "3e993e93-d3be-4586-f3a5-1a9a392a7603"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ARTICLES_PREPOSITIONS = {\n",
        "    \"english\": ['the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of']\n",
        "}\n",
        "\n",
        "NEGATION_WORDS = {\n",
        "    \"english\": [\n",
        "        'no',\n",
        "        'nor',\n",
        "        'not',\n",
        "        'don',\n",
        "        \"dont\",\n",
        "        'ain',\n",
        "        'aren',\n",
        "        \"arent\",\n",
        "        'couldn',\n",
        "        \"couldnt\",\n",
        "        'didn',\n",
        "        \"didnt\",\n",
        "        'doesn',\n",
        "        \"doesnt\",\n",
        "        'hadn',\n",
        "        \"hadnt\",\n",
        "        'hasn',\n",
        "        \"hasnt\",\n",
        "        'haven',\n",
        "        \"havent\",\n",
        "        'isn',\n",
        "        \"isnt\",\n",
        "        'mightn',\n",
        "        \"mightnt\",\n",
        "        'mustn',\n",
        "        \"mustnt\",\n",
        "        'needn',\n",
        "        \"neednt\",\n",
        "        'shan',\n",
        "        \"shant\",\n",
        "        'shouldn',\n",
        "        \"shouldnt\",\n",
        "        'wasn',\n",
        "        \"wasnt\",\n",
        "        'weren',\n",
        "        \"werent\",\n",
        "        'won',\n",
        "        \"wont\",\n",
        "        'wouldn',\n",
        "        \"wouldnt\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "PUNCTUATION = [\".\", \",\", \"'\", '\"', \"!\", \"?\", \";\", \":\", \"-\", \"(\", \")\",\"[\",\"]\",\"{\",\"}\"] #now also removes parantheses, brackets, and braces\n",
        "\n",
        "def trim(\n",
        "    text: str, stemmer: Optional[str] = None, language: str = \"english\", remove_spaces: bool = True,\n",
        "        remove_stopwords: bool = True, remove_punctuation: bool = True) -> str:\n",
        "\n",
        "    if language not in stopwords.fileids():\n",
        "        raise ValueError(\"Unsupported language\")\n",
        "\n",
        "    accepted_stemmers = (\"snowball\", \"porter\", \"lancaster\")\n",
        "    if stemmer and stemmer not in accepted_stemmers:\n",
        "        raise ValueError(\"Stemmer must be one of\", accepted_stemmers)\n",
        "\n",
        "    # merge contractions\n",
        "    text: str = text.replace(\"'\", \"\").replace(\"’\", \"\")\n",
        "\n",
        "    # tokenize words, keep uppercase\n",
        "    tokenized: List = nltk.word_tokenize(text)\n",
        "\n",
        "    if remove_punctuation:\n",
        "        tokenized = [word for word in tokenized if word not in PUNCTUATION]\n",
        "\n",
        "    if remove_stopwords:\n",
        "        nltk_stopwords = stopwords.words(language)\n",
        "        words_to_exclude = set(\n",
        "            nltk_stopwords + ARTICLES_PREPOSITIONS.get(language, [])\n",
        "        ) - set(NEGATION_WORDS.get(language, []))\n",
        "\n",
        "        tokenized = [word for word in tokenized if word.lower() not in words_to_exclude]\n",
        "\n",
        "    words = tokenized\n",
        "\n",
        "    if stemmer:\n",
        "        if stemmer == \"porter\":\n",
        "            stemmer = PorterStemmer()\n",
        "        elif stemmer == \"snowball\":\n",
        "            stemmer = SnowballStemmer(language)\n",
        "        elif stemmer == \"lancaster\":\n",
        "            stemmer = LancasterStemmer()\n",
        "        words = [stemmer.stem(word) for word in tokenized]\n",
        "\n",
        "        # restore title_case and uppercase after stemming\n",
        "        case_restored = []\n",
        "        for i, word in enumerate(words):\n",
        "            if tokenized[i].istitle():\n",
        "                word = word.title()\n",
        "            elif tokenized[i].isupper():\n",
        "                word = word.upper()\n",
        "            case_restored.append(word)\n",
        "\n",
        "        words = case_restored\n",
        "    #delete the last period \n",
        "    words2 = words.pop()\n",
        "\n",
        "    # remove spaces\n",
        "    #join_str = \"\" if remove_spaces else \" \"\n",
        "    #trimmed: str = join_str.join(words).strip()\n",
        "    #if not remove_punctuation:\n",
        "        # this is a hack to remove spaces before punctuation\n",
        "        #trimmed = re.sub(r\"\\s([?.!,:;])\", r\"\\1\", trimmed)\n",
        "        \n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "rAswfSGKonqq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text is from the input above\n",
        "trimmed_text = trim(text)\n",
        "print(trimmed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJjSEcZDo0PS",
        "outputId": "47683680-f2c7-4f9f-b37c-20115daa06f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023 Indian Premier League also known Tata IPL 2023 sponsorship reasons sometimes referred IPL 2023 IPL 16 16th season Indian Premier franchise Twenty20 cricket league India organised Board Control Cricket 1 final Super Kings defeated Gujarat Titans five wickets DLS method win record fifth league title\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "f8Uew0qs7tBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#see if there is a valid number of tokens \n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "gwuQnaS17vKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numTokens = len(encoding.encode(trimmed_text))"
      ],
      "metadata": {
        "id": "L4B5qKtA7y9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just for comparison with original \n",
        "len(encoding.encode(text))"
      ],
      "metadata": {
        "id": "4wk_f72d8bRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if numTokens >= 4096:\n",
        "  print('TOO MANY TOKENS')\n",
        "else:\n",
        "  print('Go ahead')"
      ],
      "metadata": {
        "id": "rfVt-XiG9rAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Langchain and GPT-3.5 turbo"
      ],
      "metadata": {
        "id": "Crdhphr34Cdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "38ecVXK74Gl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "wukJu-4D4MiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization\n"
      ],
      "metadata": {
        "id": "-cBD2a7COStG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chatGPT = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "hOWxZeP44V1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert at making strong factual summarizations.\\\n",
        "     Take the article submitted by the user and produce a factual useful summary\"),\n",
        "    HumanMessage(content=trimmed_text)\n",
        "]\n",
        "responses = chatGPT(messages)"
      ],
      "metadata": {
        "id": "EJXo5qg04z3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summarizing the text (something with langchain) \n",
        "summarizedText = responses\n",
        "print(summarizedText)\n",
        "\n"
      ],
      "metadata": {
        "id": "w6eGOH2jk4kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion to realistic text to speech and read out \n",
        "\n",
        "(can take a while) "
      ],
      "metadata": {
        "id": "cW3KEC4PCA5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "w_GKdjSvR7I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#long form generation\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "from IPython.display import Audio\n",
        "import nltk  # we'll use this to split into sentences\n",
        "import numpy as np\n",
        "\n",
        "from bark.generation import (\n",
        "    generate_text_semantic,\n",
        "    preload_models,\n",
        ")\n",
        "from bark.api import semantic_to_waveform\n",
        "from bark import generate_audio, SAMPLE_RATE\n",
        "\n"
      ],
      "metadata": {
        "id": "Cl01EACYRdgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preload_models()"
      ],
      "metadata": {
        "id": "qGSEorjKkr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script = summarizedText.replace(\"\\n\", \" \").strip()\n",
        "sentences = nltk.sent_tokenize(script)\n",
        "GEN_TEMP = 0.6\n",
        "SPEAKER = \"v2/en_speaker_6\"\n",
        "silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
        "\n",
        "pieces = []\n",
        "for sentence in sentences:\n",
        "    semantic_tokens = generate_text_semantic(\n",
        "        sentence,\n",
        "        history_prompt=SPEAKER,\n",
        "        temp=GEN_TEMP,\n",
        "        min_eos_p=0.05,  # this controls how likely the generation is to end\n",
        "    )\n",
        "\n",
        "    audio_array = semantic_to_waveform(semantic_tokens, history_prompt=SPEAKER,)\n",
        "    pieces += [audio_array, silence.copy()]\n",
        "    Audio(np.concatenate(pieces), rate=SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "ghvztaiTk917"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}